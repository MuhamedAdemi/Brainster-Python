{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhamedAdemi/Brainster-Python/blob/main/MuhamedAdemi_Workshop_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "unique-allah",
      "metadata": {
        "id": "unique-allah"
      },
      "source": [
        "# Workshop #3: Multivariable Calculus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "korean-thanksgiving",
      "metadata": {
        "id": "korean-thanksgiving"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import sympy as sp\n",
        "import scipy.optimize as opt\n",
        "import scipy.integrate as spi\n",
        "from scipy.optimize import minimize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "essential-impression",
      "metadata": {
        "id": "essential-impression"
      },
      "source": [
        "## Problem 1\n",
        "Find the critical points of the function $f(x, y) = x^2 + y^4 - 4xy$ by using first partial derivatives. Then use second partial derivatives to establish whether each critical point is a local minimum, a local maximum, or a saddle point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "powerful-montgomery",
      "metadata": {
        "id": "powerful-montgomery",
        "outputId": "622fd519-94d8-473f-fb9b-55d097371557",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Critical Points: [(-1, -1), (0, 0), (1, 1), (-I, I), (I, -I), (sqrt(2)*(-1 - I)/2, sqrt(2)/2 - sqrt(2)*I/2), (sqrt(2)*(-1 + I)/2, sqrt(2)/2 + sqrt(2)*I/2), (sqrt(2)*(1 - I)/2, -sqrt(2)/2 - sqrt(2)*I/2), (sqrt(2)*(1 + I)/2, -sqrt(2)/2 + sqrt(2)*I/2)]\n",
            "First Critical Point: (-1, -1) - Local Minimum\n",
            "Second Critical Point: (0, 0) - Saddle Point\n",
            "Third Critical Point: (1, 1) - Local Minimum\n"
          ]
        }
      ],
      "source": [
        "# Define the variables and the function\n",
        "x, y = sp.symbols('x y')\n",
        "f = x**4 + y**4 - 4*x*y + 1\n",
        "\n",
        "# Differentiate\n",
        "f_x = sp.diff(f, x)\n",
        "f_y = sp.diff(f, y)\n",
        "\n",
        "# Find critical points\n",
        "critical_points = sp.solve([f_x, f_y], (x, y), dict=True)\n",
        "critical_points = [(cp[x], cp[y]) for cp in critical_points]  # Convert to tuple list\n",
        "print(\"Critical Points:\", critical_points)\n",
        "\n",
        "# Classifying the critical points\n",
        "# Getting the second derivatives\n",
        "f_xx = sp.diff(f_x, x)\n",
        "f_xy = sp.diff(f_x, y)\n",
        "f_yy = sp.diff(f_y, y)\n",
        "\n",
        "# Function to classify a critical point\n",
        "def classify_critical_point(cp, f_xx, f_xy, f_yy):\n",
        "    D = f_xx.subs({x: cp[0], y: cp[1]}) * f_yy.subs({x: cp[0], y: cp[1]}) - (f_xy.subs({x: cp[0], y: cp[1]}))**2\n",
        "    f_xx_val = f_xx.subs({x: cp[0], y: cp[1]})\n",
        "    if D < 0:\n",
        "        return \"Saddle Point\"\n",
        "    elif D > 0:\n",
        "        return \"Local Minimum\" if f_xx_val > 0 else \"Local Maximum\"\n",
        "    else:\n",
        "        return \"Inconclusive\"\n",
        "\n",
        "# Working with the first critical point\n",
        "cp1 = critical_points[0]\n",
        "classification_cp1 = classify_critical_point(cp1, f_xx, f_xy, f_yy)\n",
        "print(f\"First Critical Point: {cp1} - {classification_cp1}\")\n",
        "\n",
        "# Working with the second critical point\n",
        "cp2 = critical_points[1]\n",
        "classification_cp2 = classify_critical_point(cp2, f_xx, f_xy, f_yy)\n",
        "print(f\"Second Critical Point: {cp2} - {classification_cp2}\")\n",
        "\n",
        "# Working with the third critical point\n",
        "cp3 = critical_points[2]\n",
        "classification_cp3 = classify_critical_point(cp3, f_xx, f_xy, f_yy)\n",
        "print(f\"Third Critical Point: {cp3} - {classification_cp3}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "linear-newman",
      "metadata": {
        "id": "linear-newman"
      },
      "source": [
        "## Problem 2\n",
        "Using the **Gradient Descent Method** with initial approximation $\\mathbf{x}_0 = (0, 0)$, find the minimum point and the minimum value of the function $g(x, y) = (1 - x + x^2)\\cdot e^{y^2} + (1 - y + y^2)\\cdot e^{x^2}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "certain-wrestling",
      "metadata": {
        "id": "certain-wrestling",
        "outputId": "9ab143f0-482d-4197-f17f-3396bee38019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Minimum Point: (0.277880, 0.277880)\n",
            "Minimum Value: 1.727011\n"
          ]
        }
      ],
      "source": [
        "# Define variables and functions\n",
        "def g(x):\n",
        "    return (1 - x[0] + x[0]**2) * np.exp(x[1]**2) + (1 - x[1] + x[1]**2) * np.exp(x[0]**2)\n",
        "\n",
        "def grad_g(x):\n",
        "    df_dx = (-1 + 2*x[0]) * np.exp(x[1]**2) + 2*x[0] * (1 - x[1] + x[1]**2) * np.exp(x[0]**2)\n",
        "    df_dy = 2*x[1] * (1 - x[0] + x[0]**2) * np.exp(x[1]**2) + (-1 + 2*x[1]) * np.exp(x[0]**2)\n",
        "    return np.array([df_dx, df_dy])\n",
        "\n",
        "# Running the Gradient Descent Method\n",
        "def gradient_descent(f, grad, x0, alpha=0.01, mode='min', max_iter=1000, tol=1e-6):\n",
        "    x = x0.copy()\n",
        "    for i in range(max_iter):\n",
        "        gradient = grad(x)\n",
        "        if np.linalg.norm(gradient) < tol:\n",
        "            break\n",
        "        if mode == 'min':\n",
        "            x = x - alpha * gradient\n",
        "        elif mode == 'max':\n",
        "            x = x + alpha * gradient\n",
        "    return x, f(x)\n",
        "\n",
        "# Initial parameters\n",
        "x0 = np.array([0.0, 0.0])\n",
        "alpha = 0.01  # Learning rate\n",
        "\n",
        "# Run Gradient Descent\n",
        "min_point, min_value = gradient_descent(g, grad_g, x0, alpha=alpha, mode='min', max_iter=10000)\n",
        "\n",
        "print(f\"Minimum Point: ({min_point[0]:.6f}, {min_point[1]:.6f})\")\n",
        "print(f\"Minimum Value: {min_value:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "functioning-event",
      "metadata": {
        "id": "functioning-event"
      },
      "source": [
        "## Problem 3\n",
        "On a certain workday, the *rate*, in tons per hour, at which unprocessed gravel arrives at a gravel processing plant is modeled by $G(t) = 90 + 45\\cdot \\cosâ¡\\left(\\frac{t^2}{18}\\right)$, where $t$ is measured in hours and $0 \\leqslant t \\leqslant 8$. At the beginning of the workday, $t = 0$, the plant has 500 tons of unprocessed gravel. During the hours of operation, $0 \\leqslant t \\leqslant 8$, the plant processes gravel at a constant rate $P(t) = 100$ tons per hour.\n",
        "* Find the total amount of unprocessed gravel that arrives at the plant during the hours of operation on this workday.\n",
        "* Is the amount of unprocessed gravel at the end of the workday (t=8) greater or smaller than amount of gravel at the beginning of the workday?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "solid-choice",
      "metadata": {
        "id": "solid-choice",
        "outputId": "4141eb4b-a586-47b7-c055-c707b29d9d71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total arrived gravel: 825.55 tons\n",
            "Final unprocessed gravel at t=8: 525.55 tons\n",
            "The amount at t=8 is greater than the initial 500 tons.\n"
          ]
        }
      ],
      "source": [
        "# Define the rate function G(t)\n",
        "def G(t):\n",
        "    return 90 + 45 * np.cos(t**2 / 18)\n",
        "\n",
        "# First bullet point: Total unprocessed gravel arrived during 0 â‰¤ t â‰¤ 8\n",
        "total_arrived, error = spi.quad(G, 0, 8)\n",
        "print(f\"Total arrived gravel: {total_arrived:.2f} tons\")\n",
        "\n",
        "# Second bullet point: Compare final unprocessed gravel with initial 500 tons\n",
        "initial_gravel = 500\n",
        "processed_total = 100 * 8  # P(t) = 100 tons/hour * 8 hours\n",
        "final_gravel = initial_gravel + total_arrived - processed_total\n",
        "\n",
        "if final_gravel > initial_gravel:\n",
        "    conclusion = \"greater\"\n",
        "else:\n",
        "    conclusion = \"smaller\"\n",
        "\n",
        "print(f\"Final unprocessed gravel at t=8: {final_gravel:.2f} tons\")\n",
        "print(f\"The amount at t=8 is {conclusion} than the initial 500 tons.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "destroyed-start",
      "metadata": {
        "id": "destroyed-start"
      },
      "source": [
        "## Problem 4\n",
        "Solve the system of equations:\n",
        "\\begin{equation}\n",
        "\\left\\{\n",
        "\\begin{array}{rcl}\n",
        "x - 2y + 3z &=& -1\\\\\n",
        "3x + 2y - 5z &=& 3\\\\\n",
        "2x - 5y + 2z &=& 0\n",
        "\\end{array}\n",
        "\\right.\n",
        "\\end{equation}\n",
        "\n",
        "using **Gradient Descent Method** applied to a function of the kind $f(x) = \\|A\\mathbf{x} - b\\|_2^2$ where $A\\mathbf{x} = b$ is the matrix equation that corresponds to the system, and $\\| \\cdot \\|_2$ is the Euclidean norm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fallen-hours",
      "metadata": {
        "id": "fallen-hours",
        "outputId": "1ff8081f-938f-478b-ceb5-3e297e7e6c52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Solution via Gradient Descent: [ 0.26086976 -0.08695634 -0.4782607 ]\n",
            "Residual squared: 0.000000\n",
            "Exact solution (np.linalg.solve): [ 0.26086957 -0.08695652 -0.47826087]\n"
          ]
        }
      ],
      "source": [
        "# Define the matrix A and vector b from the system Ax = b\n",
        "A = np.array([[1, -2, 3],\n",
        "              [3, 2, -5],\n",
        "              [2, -5, 2]], dtype=np.float64)\n",
        "b = np.array([-1, 3, 0], dtype=np.float64)\n",
        "\n",
        "# Define the residual squared function and its gradient\n",
        "def f(x):\n",
        "    residual = A @ x - b\n",
        "    return np.sum(residual ** 2)\n",
        "\n",
        "def grad_f(x):\n",
        "    return 2 * A.T @ (A @ x - b)  # Gradient: 2Aáµ€(Ax - b)\n",
        "\n",
        "# Gradient Descent Implementation\n",
        "def gradient_descent(f, grad, x0, alpha=0.001, max_iter=10000, tol=1e-6):\n",
        "    x = x0.copy()\n",
        "    for _ in range(max_iter):\n",
        "        gradient = grad(x)\n",
        "        if np.linalg.norm(gradient) < tol:  # Check convergence\n",
        "            break\n",
        "        x -= alpha * gradient  # Update step\n",
        "    return x, f(x)\n",
        "\n",
        "# Initial guess and parameters\n",
        "x0 = np.zeros(3)  # Start at [0, 0, 0]\n",
        "alpha = 0.001     # Learning rate (adjust empirically)\n",
        "\n",
        "# Run Gradient Descent\n",
        "solution, min_value = gradient_descent(f, grad_f, x0, alpha=alpha)\n",
        "print(f\"Solution via Gradient Descent: {solution}\")\n",
        "print(f\"Residual squared: {min_value:.6f}\")\n",
        "\n",
        "# Verify with direct linear solver\n",
        "direct_solution = np.linalg.solve(A, b)\n",
        "print(f\"Exact solution (np.linalg.solve): {direct_solution}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7pozFzUeQB1f",
      "metadata": {
        "id": "7pozFzUeQB1f"
      },
      "source": [
        "## Problem 5\n",
        "\n",
        "[Use Gradient Descent Method] Four people are typing two paragraphs of text. The first paragraph uses elementary vocabulary, while the second paragraph contains some more advanced words. The number of typos that the people make are labeled by ð‘¥, for the first paragraph, and ð‘¦, for the second paragraph. Data collected is given in the accompanying table:\n",
        "\n",
        "| x \t | y \t|\n",
        "|---\t |---\t|\n",
        "| 1 \t | 4 \t|\n",
        "| 3 \t | 5 \t|\n",
        "| 4 \t | 6 \t|\n",
        "| 5 \t | 8 \t|\n",
        "\n",
        "\n",
        "We will build a model to predict the number of typos $y$ in the second text based on the number of typos in the first test. To do this, we use least-squares regression, an approach similar to the one we used for systems. For every number of observed typos $x_i$ for the first text, we have a corresponding number $y_i$ of observed typos for the second text. Our model $\\hat{y_i} = f(x_i)$ will make a prediction of the number of typos on the second text. Most often, the observed number of typos $y_i$ and the predicted number of typos $\\hat{y_i}$ will not be equal.\n",
        "Let us label each such discrepancy in values by $d_i$, in other words:\n",
        "\n",
        "\\begin{align}\n",
        "d_i = observed_i - predicted_i = y_i - \\hat{y_i}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "We call these discrepancies residuals. The goal now is to choose a model which will minimize the total sum of the squares of the residuals, i.e. a model which will make the overall discrepancy between the observed data and the predictions based on it as small as possible. Note that the form of the residuals depends on the choice of the model. For example, choosing a linear model $\\hat{y_i}=a+bx$, we have prediction $\\hat{y_i}=a+bx$, so the residuals become:\n",
        "\n",
        "\\begin{align}\n",
        "d^2 = (y-\\hat{y})^2 = (y-a-bx)^2\n",
        "\\end{align}\n",
        "\n",
        "The total sum of the squares of the residuals is then $\\sum_{i=1}^n d_i^2 = \\sum_{i=1}^n (y_i-a-bx_i)^2$. This allows us to define the function $f$ that we need to minimize in order to build the model as:\n",
        "\n",
        "\\begin{align}\n",
        "f(a,b) = \\sum_{i=1}^n(y_i-a-bx_i)^2\n",
        "\\end{align}\n",
        "\n",
        "By minimizing this function, we find the values for the coefficients $a$ and $b$ in the model that minimize the discrepancy between the data and the type of model we chose to work with.\n",
        "\n",
        "*a)* Build a linear model for the data: $\\hat{y}=a+bx$. Then make a prediction about the number of typos $y$ a person would make when typing the second text if they have made $x=2$ typos when typing the first text. **Use your own gradient_descent function.**\n",
        "\n",
        "*b)* Build a quadratic model for the data: $\\hat{y}=a+bx+cx^2$. Then make a prediction about the same person from part a). **Use the minimizer BFGS.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qCNOYmmxUjzO",
      "metadata": {
        "id": "qCNOYmmxUjzO"
      },
      "source": [
        "**a) Linear model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tDW7nTRgP-JD",
      "metadata": {
        "id": "tDW7nTRgP-JD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Data points (x, y)\n",
        "X = np.array([1, 3, 4, 5])  # Independent variable (number of typos in first paragraph)\n",
        "Y = np.array([4, 5, 6, 8])  # Dependent variable (number of typos in second paragraph)\n",
        "\n",
        "# Define the cost function (Mean Squared Error)\n",
        "def f_func(params):\n",
        "    m, b = params\n",
        "    predictions = m * X + b\n",
        "    return np.sum((predictions - Y) ** 2) / len(X)  # Mean Squared Error (MSE)\n",
        "\n",
        "# Define the gradient function for gradient descent\n",
        "def grad_f(params):\n",
        "    m, b = params\n",
        "    n = len(X)\n",
        "    predictions = m * X + b\n",
        "    dm = (-2/n) * np.sum(X * (Y - predictions))  # Partial derivative w.r.t m\n",
        "    db = (-2/n) * np.sum(Y - predictions)        # Partial derivative w.r.t b\n",
        "    return np.array([dm, db])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baf12071-00e2-486a-803f-f43c9be40919",
      "metadata": {
        "id": "baf12071-00e2-486a-803f-f43c9be40919",
        "outputId": "324ca9b4-f58f-4d5d-cd27-df6ca377a701"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal parameters (m, b): [0.96780827 2.58906494]\n",
            "Predicted typos for x=2: 4.524681485644655\n"
          ]
        }
      ],
      "source": [
        "# Initialize parameters (m and b)\n",
        "params = np.random.randn(2)\n",
        "\n",
        "# Gradient descent parameters\n",
        "alpha = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# Perform Gradient Descent\n",
        "for _ in range(epochs):\n",
        "    params -= alpha * grad_f(params)\n",
        "\n",
        "solution_vector = params\n",
        "print(f\"Optimal parameters (m, b): {solution_vector}\")\n",
        "\n",
        "# Prediction for x = 2\n",
        "x_new = 2\n",
        "y_pred = solution_vector[0] * x_new + solution_vector[1]\n",
        "print(f\"Predicted typos for x={x_new}: {y_pred}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YzzxuAtIUsC6",
      "metadata": {
        "id": "YzzxuAtIUsC6"
      },
      "source": [
        "**b) Quadratic model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E-7Kct6HUikW",
      "metadata": {
        "id": "E-7Kct6HUikW"
      },
      "outputs": [],
      "source": [
        "# Define variables and matrices\n",
        "\n",
        "\n",
        "# Define the function f\n",
        "\n",
        "\n",
        "# Define the numpy function f\n",
        "\n",
        "#def f_func(x0):\n",
        "#   return\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define quadratic model y = a*x^2 + b*x + c\n",
        "def f_func(params):\n",
        "    a, b, c = params\n",
        "    predictions = a * X**2 + b * X + c\n",
        "    return np.sum((predictions - Y)**2)  # Sum of Squared Errors (SSE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2-_fEnpgUxaS",
      "metadata": {
        "id": "2-_fEnpgUxaS"
      },
      "outputs": [],
      "source": [
        "# Initial guess for parameters [a, b, c]\n",
        "initial_params = np.random.randn(3)\n",
        "\n",
        "# Optimize using BFGS\n",
        "result = minimize(f_func, initial_params, method='BFGS')\n",
        "\n",
        "# Extract optimal parameters\n",
        "solution_vector = result.x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KLWXNrVpV-xg",
      "metadata": {
        "id": "KLWXNrVpV-xg",
        "outputId": "82bcb89f-a5c3-4d6c-d8f6-8b864b99e9da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal parameters (a, b, c): [ 0.27272692 -0.65454339  4.39999808]\n",
            "Predicted typos for x=2: 4.181818968173459\n"
          ]
        }
      ],
      "source": [
        "print(f\"Optimal parameters (a, b, c): {solution_vector}\")\n",
        "\n",
        "# Prediction for x = 2\n",
        "x_new = 2\n",
        "y_pred_quad = solution_vector[0] * x_new**2 + solution_vector[1] * x_new + solution_vector[2]\n",
        "print(f\"Predicted typos for x={x_new}: {y_pred_quad}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ncOkIOkEWyGF",
      "metadata": {
        "id": "ncOkIOkEWyGF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}